<?xml version='1.0' encoding='UTF-8' standalone='no'?>
<doxygen xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="compound.xsd" version="1.10.0" xml:lang="en-US">
  <compounddef id="namespacegpt__researcher_1_1utils_1_1llm" kind="namespace" language="Python">
    <compoundname>gpt_researcher::utils::llm</compoundname>
    <sectiondef kind="func">
      <memberdef kind="function" id="namespacegpt__researcher_1_1utils_1_1llm_1a350bb4294397495c506426a2caad2b31" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type></type>
        <definition>gpt_researcher.utils.llm.get_llm</definition>
        <argsstring>(llm_provider, **kwargs)</argsstring>
        <name>get_llm</name>
        <qualifiedname>gpt_researcher.utils.llm.get_llm</qualifiedname>
        <param>
          <type>llm_provider</type>
          <defname>llm_provider</defname>
        </param>
        <param>
          <type>**</type>
          <declname>kwargs</declname>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" line="18" column="1" bodyfile="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" bodystart="18" bodyend="54"/>
        <referencedby refid="namespacegpt__researcher_1_1utils_1_1llm_1a3125e642ba0eba1562845ff7d15ecb08" compoundref="llm_8py" startline="107" endline="140">gpt_researcher.utils.llm.construct_subtopics</referencedby>
        <referencedby refid="namespacegpt__researcher_1_1utils_1_1llm_1aabaadb598e9e86c6e4a0c50627463b95" compoundref="llm_8py" startline="65" endline="106">gpt_researcher.utils.llm.create_chat_completion</referencedby>
      </memberdef>
      <memberdef kind="function" id="namespacegpt__researcher_1_1utils_1_1llm_1aabaadb598e9e86c6e4a0c50627463b95" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>str</type>
        <definition> str gpt_researcher.utils.llm.create_chat_completion</definition>
        <argsstring>(list messages, Optional[str] model=None, float temperature=1.0, Optional[int] max_tokens=None, Optional[str] llm_provider=None, Optional[bool] stream=False, WebSocket|None websocket=None, Dict[str, Any]|None llm_kwargs=None, callable cost_callback=None)</argsstring>
        <name>create_chat_completion</name>
        <qualifiedname>gpt_researcher.utils.llm.create_chat_completion</qualifiedname>
        <param>
          <type>list</type>
          <declname>messages</declname>
        </param>
        <param>
          <type>Optional</type>
          <declname>model</declname>
          <array>[str]</array>
          <defval>None</defval>
        </param>
        <param>
          <type>float</type>
          <declname>temperature</declname>
          <defval>1.0</defval>
        </param>
        <param>
          <type>Optional</type>
          <declname>max_tokens</declname>
          <array>[int]</array>
          <defval>None</defval>
        </param>
        <param>
          <type>Optional</type>
          <declname>llm_provider</declname>
          <array>[str]</array>
          <defval>None</defval>
        </param>
        <param>
          <type>Optional</type>
          <declname>stream</declname>
          <array>[bool]</array>
          <defval>False</defval>
        </param>
        <param>
          <type>WebSocket|None</type>
          <declname>websocket</declname>
          <defval>None</defval>
        </param>
        <param>
          <type>Dict|None</type>
          <declname>llm_kwargs</declname>
          <array>[str, Any]</array>
          <defval>None</defval>
        </param>
        <param>
          <type>callable</type>
          <declname>cost_callback</declname>
          <defval>None</defval>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
<para><verbatim>Create a chat completion using the OpenAI API
Args:
    messages (list[dict[str, str]]): The messages to send to the chat completion
    model (str, optional): The model to use. Defaults to None.
    temperature (float, optional): The temperature to use. Defaults to 0.9.
    max_tokens (int, optional): The max tokens to use. Defaults to None.
    stream (bool, optional): Whether to stream the response. Defaults to False.
    llm_provider (str, optional): The LLM Provider to use.
    webocket (WebSocket): The websocket used in the currect request,
    cost_callback: Callback function for updating cost
Returns:
    str: The response from the chat completion
</verbatim> </para>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" line="55" column="1" bodyfile="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" bodystart="65" bodyend="106"/>
        <references refid="namespacegpt__researcher_1_1utils_1_1llm_1a350bb4294397495c506426a2caad2b31" compoundref="llm_8py" startline="18" endline="54">gpt_researcher.utils.llm.get_llm</references>
      </memberdef>
      <memberdef kind="function" id="namespacegpt__researcher_1_1utils_1_1llm_1a3125e642ba0eba1562845ff7d15ecb08" prot="public" static="no" const="no" explicit="no" inline="no" virt="non-virtual">
        <type>list</type>
        <definition> list gpt_researcher.utils.llm.construct_subtopics</definition>
        <argsstring>(str task, str data, config, list subtopics=[])</argsstring>
        <name>construct_subtopics</name>
        <qualifiedname>gpt_researcher.utils.llm.construct_subtopics</qualifiedname>
        <param>
          <type>str</type>
          <declname>task</declname>
        </param>
        <param>
          <type>str</type>
          <declname>data</declname>
        </param>
        <param>
          <type>config</type>
          <defname>config</defname>
        </param>
        <param>
          <type>list</type>
          <declname>subtopics</declname>
          <defval>[]</defval>
        </param>
        <briefdescription>
        </briefdescription>
        <detaileddescription>
        </detaileddescription>
        <inbodydescription>
        </inbodydescription>
        <location file="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" line="107" column="1" bodyfile="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" bodystart="107" bodyend="140"/>
        <references refid="namespacegpt__researcher_1_1utils_1_1llm_1a350bb4294397495c506426a2caad2b31" compoundref="llm_8py" startline="18" endline="54">gpt_researcher.utils.llm.get_llm</references>
        <referencedby refid="classgpt__researcher_1_1master_1_1agent_1_1GPTResearcher_1a245afde9b9be308fd5ac526d4052fede" compoundref="gpt__researcher_2master_2agent_8py" startline="301" endline="324">gpt_researcher.master.agent.GPTResearcher.get_subtopics</referencedby>
      </memberdef>
    </sectiondef>
    <briefdescription>
    </briefdescription>
    <detaileddescription>
    </detaileddescription>
    <location file="/tmp/github_repos_arch_doc_gen/yshishenya/rob/gpt_researcher/utils/llm.py" line="1" column="1"/>
  </compounddef>
</doxygen>
